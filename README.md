# Live Code Assessment

## Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/weareinto/live-code-assessment.git
cd live-code-assessment
```

2. **Copy the `.env.sample` file to `.env`**

```bash
cp .env.sample .env
```

3. **Create and activate a virtual environment**

```bash
python -m venv venv
source venv/bin/activate    # On Windows use: venv\Scripts\activate
```

4. **Install dependencies**

```bash
pip install -r requirements.txt
```

5. **Run the FastAPI app**

```bash
uvicorn main:app --reload
```

6. **Access the app**

* Open your browser and go to [http://localhost:8000](http://localhost:8000)
* API docs are at [http://localhost:8000/docs](http://localhost:8000/docs)

---

## üìã Task Implementation Order
Complete the tasks in this specific order for optimal workflow:

### **Step 1: Start with AI Logic** ü§ñ
### **Step 2: Implement Vector Retrieval** üóÑÔ∏è  
### **Step 3: Configuration Setup** üìù
### **Step 4: API Layer** üöÄ
### **Step 5: Background Tasks (BONUS)** ‚ö°

---

## üìã Task Breakdown by File

### 1. ü§ñ **ai.py** - AI Response Generation (START HERE)
**What to implement:**
- Complete the `generate_response()` method to inject context keys into the PROMPT
- Implement the `__retrieve()` method to get the top 1 tweet using the retriever
- Process the context dictionary to replace placeholders (`{topic}`, `{location}`, `{language}`) in PROMPT

**Requirements:**
- Use the retriever to get the most relevant tweet based on the injected prompt
- Return both the tweet content and author information
- Handle the context parameter to inject values into the PROMPT template

---

### 2. üóÑÔ∏è **retriever.py** - Vector Search 
**What to implement:**
- Load tweets from `data/tweets.json`
- Create Document objects with tweet text as page_content and author info as metadata
- Populate the `tweets` list to initialize the FAISS vector store

**Requirements:**
- Load JSON data and convert to LangChain Documents
- Set tweet text as page content and author info as metadata
- Use existing FAISS and FakeEmbeddings setup
- Make retriever available for [`FakeAI`](ai.py) class integration

---

### 3. üìù **config.py** - Settings Management
**What to implement:**
- Create a FastAPI-compatible settings class using `pydantic-settings`
- Load environment variables from `.env` file (`X_API_KEY`)
- Replace the `None` assignment to `settings` with the actual settings instance

**Requirements:**
- Use `BaseSettings` from `pydantic_settings`
- Load `X_API_KEY` from environment variables
- Make settings available throughout the app

---

### 4. üöÄ **main.py** & üìä **schemas.py** - API Implementation
**What to implement in schemas.py:**
- Create Pydantic models for API request and response validation
- Reference the [`Tweet`](models.py) model structure for consistency

**What to implement in main.py:**
- Create POST `/generate` endpoint for generating AI responses and saving to database
- Create GET `/tweets` endpoint to retrieve all tweets sorted by author name (ascending) and created_at (descending)
- Use the [`FakeAI`](ai.py) class and [`Tweet`](models.py) model
- Implement proper database integration with [`get_db`](models.py) dependency

**üìñ Note:** [`models.py`](models.py) is **READ-ONLY** - do not edit this file. It contains the complete SQLAlchemy setup and [`Tweet`](models.py) model that you should use as-is.

**üîê Authentication & Payload Requirements:**
- **POST `/generate` endpoint payload** must include all 3 context variables (all required):
  - `topic` (string)
  - `location` (string) 
  - `language` (string)
- **Authentication**: Users must pass `X-API-Key` header to authenticate against the `X_API_KEY` environment variable

**Required schemas:**
- Request model for tweet generation (with `topic`, `location`, `language` fields)
- Response model for tweet generation (Pydantic schema matching the saved [`Tweet`](models.py) object)
- Response model for tweet retrieval (list of Pydantic schemas similar to [`Tweet`](models.py) model structure)
- Proper validation and serialization

**Endpoint specifications:**
- **POST `/generate` endpoint**: 
  - **Payload**: Requires all 3 context variables: `topic`, `location`, `language` (all required strings)
  - **Authentication**: Requires `X-API-Key` header matching `X_API_KEY` environment variable
  - Uses [`FakeAI`](ai.py) to generate response with context injection
  - Saves to database using [`Tweet`](models.py) model
  - **Returns**: The saved tweet object as Pydantic schema (matching the database record)
- **GET `/tweets` endpoint**: 
  - Retrieves all tweets from database
  - Sorts by author_name (ascending) and created_at (descending)
  - **Returns**: List of Pydantic schemas (similar to [`Tweet`](models.py) model structure)

---

### 5. ‚ö° **background.py** - Background Tasks (BONUS)
**What to implement:**
- Create FastAPI background task function
- After generating AI response and saving to database, check for duplicate authors
- If a tweet exists with the same author email, delete the oldest tweet for that author
- Integrate with the main tweet generation endpoint

**Requirements:**
- Use FastAPI background tasks
- Check for tweets with matching author email
- Delete oldest tweet when duplicates exist
- Run after successful tweet generation and database save

---

## üéØ Implementation Requirements Summary

### **REQUIRED (Must complete in order):**
1. [ ] **AI Response Generation** ([`ai.py`](ai.py))
   - Context injection into PROMPT template
   - Integration with retriever for tweet selection

2. [ ] **Vector Search** ([`retriever.py`](retriever.py))
   - Load and process tweet data from [`data/tweets.json`](data/tweets.json)
   - Create searchable vector store with FAISS

3. [ ] **Settings Management** ([`config.py`](config.py))
   - Environment variable loading with pydantic-settings

4. [ ] **API Endpoints & Schemas** ([`main.py`](main.py) & [`schemas.py`](schemas.py))
   - POST `/generate` endpoint for tweet generation using [`FakeAI`](ai.py)
   - GET `/tweets` endpoint for tweet retrieval with proper sorting
   - Pydantic models for request/response validation
   - Database integration using [`Tweet`](models.py) model (READ-ONLY - do not edit [`models.py`](models.py))

### **BONUS (Optional for extra credit):**
5. [ ] **Background Tasks** ([`background.py`](background.py))
   - Automatic cleanup of duplicate author tweets
   - Background processing integration

---

## üîß Implementation Notes

- **Database**: The [`Tweet`](models.py) model is ready to use with SQLAlchemy setup (**READ-ONLY** - do not modify [`models.py`](models.py))
- **Vector Store**: Use the existing FAISS and FakeEmbeddings configuration
- **Data Source**: Tweet data is available in [`data/tweets.json`](data/tweets.json)
- **Testing**: Use FastAPI's automatic documentation at `/docs` to test your endpoints
- **Order Matters**: Follow the step-by-step order for smooth implementation